# naver_place_reviews.py
"""
Naver Place Review Crawler (mobile)
-----------------------------------
ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤(ëª¨ë°”ì¼) ë¦¬ë·°ë¥¼ í¬ë¡¤ë§í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤.

ğŸ“Œ í•µì‹¬ í¬ì¸íŠ¸
- ğŸš¨ ê°€ê²Œë§ˆë‹¤ ê¼­ ë°”ê¿”ì•¼ í•˜ëŠ” ê°’: place_id
  - ì˜ˆì‹œ URL: https://map.naver.com/v5/entry/place/36978606
  - ì—¬ê¸°ì„œ 36978606 ì´ place_id ì…ë‹ˆë‹¤.
  - í¬ë¡¤ë§í•  ë•Œë§ˆë‹¤ ì›í•˜ëŠ” ê°€ê²Œì˜ place_idë¡œ ë³€ê²½í•˜ì„¸ìš”.

ğŸ“‹ ê¸°ëŠ¥
- ëª¨ë°”ì¼ ë¦¬ë·° í˜ì´ì§€ ì—´ê¸° (/restaurant, /place ë‘ ê²½ìš° ëª¨ë‘ ì‹œë„)
- "ë”ë³´ê¸°" ë²„íŠ¼ ìë™ í´ë¦­ ë°˜ë³µ
- ë‹‰ë„¤ì„, ë¦¬ë·° ë³¸ë¬¸, ì‘ì„± ë‚ ì§œ, ì¬ë°©ë¬¸ ì—¬ë¶€ ì¶”ì¶œ
- ê²°ê³¼ë¥¼ CSV(UTF-8-SIG)ë¡œ ì €ì¥

ğŸ’» ì‹¤í–‰ ì˜ˆì‹œ
  pip install selenium webdriver-manager beautifulsoup4 lxml

  # ê¸°ë³¸ ì‹¤í–‰
  python naver_place_reviews.py --place_id 36978606

  # ì˜µì…˜ ì¶”ê°€ ì‹¤í–‰
  python naver_place_reviews.py --place_id 36978606 --max_clicks 60 --headless
  python naver_place_reviews.py --place_id 36978606 --sort recent   # (recent/favorite/relevance)
"""


from __future__ import annotations
import time
import csv
import sys
import argparse
import json
import datetime as dt
from pathlib import Path
from typing import List, Dict

from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException
from webdriver_manager.chrome import ChromeDriverManager


# -----------------------------
# URL & Driver
# -----------------------------
def build_review_urls(place_id: str, sort: str = "recent") -> List[str]:
    """
    ì—…ì¢… ìŠ¤ì½”í”„ì— ë”°ë¼ /restaurant/{id} ë˜ëŠ” /place/{id}ê°€ ì—´ë¦´ ìˆ˜ ìˆì–´ ë‘˜ ë‹¤ ì‹œë„.
    """
    scopes = ["restaurant", "place"]
    return [
        f"https://m.place.naver.com/{sc}/{place_id}/review/visitor?entry=ple&reviewSort={sort}"
        for sc in scopes
    ]


def make_driver(headless: bool = False) -> webdriver.Chrome:
    opts = webdriver.ChromeOptions()
    if headless:
        opts.add_argument("headless=new")
    opts.add_argument("window-size=1920,1080")
    opts.add_argument("disable-gpu")
    # ë¬´ë‚œí•œ UA
    opts.add_argument(
        "user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    )
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)
    driver.implicitly_wait(10)
    return driver


# -----------------------------
# Interactions
# -----------------------------
def click_more_until_end(driver: webdriver.Chrome, max_clicks: int = 50, sleep_sec: float = 0.4):
    """
    'ë”ë³´ê¸°' ë²„íŠ¼ì„ ìµœëŒ€ max_clicks ë²ˆê¹Œì§€ ë°˜ë³µ í´ë¦­. í˜ì´ì§€/ì‹œì ì— ë”°ë¼ DOMì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆì–´
    ì—¬ëŸ¬ ì…€ë ‰í„° ì „ëµì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„.
    """
    # ìŠ¤í¬ë¡¤/ë Œë”ë§ ìœ ë„
    try:
        driver.find_element(By.TAG_NAME, "body").send_keys(Keys.PAGE_DOWN)
        time.sleep(0.4)
    except Exception:
        pass

    # ìš°ì„  ì‹œë„í•  XPATH (ì‹œê¸°/ì¼€ì´ìŠ¤ë³„ë¡œ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
    xpath_candidates = [
        '//*[@id="app-root"]//a[contains(.,"ë”ë³´ê¸°")]',
        '//*[@id="app-root"]//button[contains(.,"ë”ë³´ê¸°")]',
        '//*[@id="app-root"]/div/div/div//a[contains(@href,"review") and contains(.,"ë”ë³´ê¸°")]',
    ]

    clicks = 0
    while clicks < max_clicks:
        clicked = False

        # 1) XPATH ìš°ì„  ì‹œë„
        for xp in xpath_candidates:
            try:
                btn = driver.find_element(By.XPATH, xp)
                if btn.is_displayed():
                    btn.click()
                    clicked = True
                    break
            except (NoSuchElementException, ElementClickInterceptedException):
                continue
            except Exception:
                continue
        if not clicked:
            # 2) CSS ì „ìˆ˜ì¡°ì‚¬: 'ë”ë³´ê¸°' í…ìŠ¤íŠ¸ í¬í•¨ ë§í¬/ë²„íŠ¼
            try:
                for el in driver.find_elements(By.CSS_SELECTOR, "a, button"):
                    try:
                        txt = (el.text or "").strip()
                        if "ë”ë³´ê¸°" in txt and el.is_displayed():
                            el.click()
                            clicked = True
                            break
                    except Exception:
                        continue
            except Exception:
                pass

        if not clicked:
            # ë” ì´ìƒ í´ë¦­í•  'ë”ë³´ê¸°'ê°€ ì—†ë‹¤ê³  íŒë‹¨
            time.sleep(0.6)
            break

        clicks += 1
        time.sleep(sleep_sec)

    return clicks


# -----------------------------
# Parsing
# -----------------------------
def parse_reviews_from_html(html: str):
    from bs4 import BeautifulSoup
    bs = BeautifulSoup(html, "lxml")
    data = []

    # ë¦¬ë·° ë¦¬ìŠ¤íŠ¸ í•­ëª©ë“¤
    items = bs.select("li.place_app") or bs.select("li")  # fallback

    for r in items:
        # âœ… ë‹‰ë„¤ì„
        nickname = r.select_one("span.pui__NMi-Dp")
        nickname = nickname.get_text(strip=True) if nickname else ""

        # âœ… ë³¸ë¬¸
        content = r.select_one("div.pui__vn15t2")
        content = content.get_text("\n", strip=True) if content else ""

        # âœ… í•´ì‹œíƒœê·¸(â€¦ìš”) â€“ ê°„ë‹¨ ë²„ì „
        box = r.select_one("div.pui__HLNvmI")
        tag = [s.get_text(strip=True) for s in box.select("span.pui__jhpEyP")] if box else []
        # (ì„ íƒ) ì¤‘ë³µ ì œê±°ê°€ í•„ìš”í•˜ë©´ ë‹¤ìŒ í•œ ì¤„ë§Œ ì¶”ê°€
        # tags = list(dict.fromkeys(tags))


        if nickname or content or tag:
            data.append({
                "nickname": nickname,
                "content": content,
                 "tags_json": json.dumps(tag, ensure_ascii=False)
            })

    return data




# -----------------------------
# Save
# -----------------------------
def save_csv(records: List[Dict[str, str]], place_id: str, out_dir: str = "./outputs") -> str:
    Path(out_dir).mkdir(parents=True, exist_ok=True)
    ts = dt.datetime.now().strftime("%Y%m%d_%H%M%S")
    path = Path(out_dir) / f"naver_place_reviews_{place_id}_{ts}.csv"
    with open(path, "w", newline="", encoding="utf-8-sig") as f:
        w = csv.DictWriter(f, fieldnames=["nickname", "content", "tags_json"])
        w.writeheader()
        for row in records:
            w.writerow(row)
    return str(path)


# -----------------------------
# Orchestration
# -----------------------------
def fetch_reviews(place_id: str, sort: str = "recent", max_clicks: int = 50, headless: bool = False) -> str:
    driver = make_driver(headless=headless)
    try:
        last_err = None
        for url in build_review_urls(place_id, sort=sort):
            try:
                driver.get(url)
                time.sleep(1.0)

                # ë¦¬ë·° í˜ì´ì§€ ì—¬ë¶€ ëŒ€ëµ íŒë³„
                if "review" not in driver.current_url.lower():
                    continue

                clicks = click_more_until_end(driver, max_clicks=max_clicks, sleep_sec=0.4)
                # ë¡œë”© ì—¬ìœ 
                time.sleep(1.2)

                html = driver.page_source
                records = parse_reviews_from_html(html)
                if records:
                    out = save_csv(records, place_id)
                    print(f"[OK] {len(records)} reviews saved ({clicks} clicks) -> {out}")
                    return out
            except Exception as e:
                last_err = e
                continue

        # ëª¨ë“  í›„ë³´ ì‹¤íŒ¨
        if last_err:
            raise last_err
        raise RuntimeError("ë¦¬ë·° í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨")
    finally:
        driver.quit()


# -----------------------------
# CLI
# -----------------------------
def main(argv=None):
    p = argparse.ArgumentParser(description="Naver Place Review Crawler (mobile)")
    p.add_argument("--place_id", required=True, help="ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ID (ì˜ˆ: 36978606)")
    p.add_argument("--sort", default="recent", help="ì •ë ¬ (recent/favorite ë“± í˜ì´ì§€ê°€ í—ˆìš©í•˜ëŠ” ê°’)")
    p.add_argument("--max_clicks", type=int, default=50, help="ë”ë³´ê¸° ìµœëŒ€ í´ë¦­ íšŸìˆ˜")
    p.add_argument("--headless", action="store_true", help="ë¸Œë¼ìš°ì € ì°½ ì—†ì´ ì‹¤í–‰")
    args = p.parse_args(argv)

    out = fetch_reviews(
        place_id=args.place_id,
        sort=args.sort,
        max_clicks=args.max_clicks,
        headless=args.headless,
    )
    print(out)


if __name__ == "__main__":
    sys.exit(main())


